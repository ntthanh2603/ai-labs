{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da0fff3f",
   "metadata": {},
   "source": [
    "# Training model LLM use Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7428ab94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  3 23:54:52 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:07:00.0 Off |                    0 |\n",
      "| N/A   54C    P0             27W /   70W |    6257MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A         1125590      C   ...e/thanh/miniconda3/bin/python       6254MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0142c3",
   "metadata": {},
   "source": [
    "#### C√†i ƒë·∫∑t th∆∞ vi·ªán "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b3d8e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-_aimmuky/unsloth_e6a29bbabafd4650adfccb523cbabbef\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-_aimmuky/unsloth_e6a29bbabafd4650adfccb523cbabbef\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 6789c279d578278aca4af22f4ca31fc42829c9a4\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: unsloth_zoo>=2025.11.6 in /home/thanh/miniconda3/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.11.6)\n",
      "Requirement already satisfied: packaging in /home/thanh/miniconda3/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.0)\n",
      "Requirement already satisfied: tyro in /home/thanh/miniconda3/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.9.35)\n",
      "Requirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3 in /home/thanh/miniconda3/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.57.1)\n",
      "Requirement already satisfied: datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1 in /home/thanh/miniconda3/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.3.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /home/thanh/miniconda3/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.1)\n",
      "Requirement already satisfied: tqdm in /home/thanh/miniconda3/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/thanh/miniconda3/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (7.1.3)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /home/thanh/miniconda3/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.1)\n",
      "Requirement already satisfied: numpy in /home/thanh/miniconda3/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.3.5)\n",
      "Requirement already satisfied: protobuf in /home/thanh/miniconda3/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.33.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.34.0 in /home/thanh/miniconda3/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.36.0)\n",
      "Requirement already satisfied: hf_transfer in /home/thanh/miniconda3/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.9)\n",
      "Requirement already satisfied: bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 in /home/thanh/miniconda3/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.48.2)\n",
      "Requirement already satisfied: filelock in /home/thanh/miniconda3/lib/python3.12/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/thanh/miniconda3/lib/python3.12/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/thanh/miniconda3/lib/python3.12/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/thanh/miniconda3/lib/python3.12/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/thanh/miniconda3/lib/python3.12/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/thanh/miniconda3/lib/python3.12/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /home/thanh/miniconda3/lib/python3.12/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/thanh/miniconda3/lib/python3.12/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /home/thanh/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/thanh/miniconda3/lib/python3.12/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/thanh/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.13.2)\n",
      "Requirement already satisfied: anyio in /home/thanh/miniconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/thanh/miniconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/thanh/miniconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/thanh/miniconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/thanh/miniconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/thanh/miniconda3/lib/python3.12/site-packages (from huggingface_hub>=0.34.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/thanh/miniconda3/lib/python3.12/site-packages (from huggingface_hub>=0.34.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/thanh/miniconda3/lib/python3.12/site-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/thanh/miniconda3/lib/python3.12/site-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/thanh/miniconda3/lib/python3.12/site-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/thanh/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/thanh/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/thanh/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/thanh/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/thanh/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/thanh/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/thanh/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.22.0)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /home/thanh/miniconda3/lib/python3.12/site-packages (from bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.1)\n",
      "Requirement already satisfied: setuptools in /home/thanh/miniconda3/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/thanh/miniconda3/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/thanh/miniconda3/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/thanh/miniconda3/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/thanh/miniconda3/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/thanh/miniconda3/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/thanh/miniconda3/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/thanh/miniconda3/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/thanh/miniconda3/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/thanh/miniconda3/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/thanh/miniconda3/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/thanh/miniconda3/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/thanh/miniconda3/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/thanh/miniconda3/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/thanh/miniconda3/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/thanh/miniconda3/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/thanh/miniconda3/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/thanh/miniconda3/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/thanh/miniconda3/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/thanh/miniconda3/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/thanh/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/thanh/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/thanh/miniconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
      "Requirement already satisfied: torchao>=0.13.0 in /home/thanh/miniconda3/lib/python3.12/site-packages (from unsloth_zoo>=2025.11.6->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.14.1)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /home/thanh/miniconda3/lib/python3.12/site-packages (from unsloth_zoo>=2025.11.6->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.12.0)\n",
      "Requirement already satisfied: trl!=0.19.0,<=0.24.0,>=0.18.2 in /home/thanh/miniconda3/lib/python3.12/site-packages (from unsloth_zoo>=2025.11.6->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.24.0)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /home/thanh/miniconda3/lib/python3.12/site-packages (from unsloth_zoo>=2025.11.6->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.18.0)\n",
      "Requirement already satisfied: cut_cross_entropy in /home/thanh/miniconda3/lib/python3.12/site-packages (from unsloth_zoo>=2025.11.6->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.1.1)\n",
      "Requirement already satisfied: pillow in /home/thanh/miniconda3/lib/python3.12/site-packages (from unsloth_zoo>=2025.11.6->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.0.0)\n",
      "Requirement already satisfied: msgspec in /home/thanh/miniconda3/lib/python3.12/site-packages (from unsloth_zoo>=2025.11.6->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.20.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/thanh/miniconda3/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/thanh/miniconda3/lib/python3.12/site-packages (from jinja2->torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/thanh/miniconda3/lib/python3.12/site-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/thanh/miniconda3/lib/python3.12/site-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/thanh/miniconda3/lib/python3.12/site-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/thanh/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.17.0)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /home/thanh/miniconda3/lib/python3.12/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.17.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /home/thanh/miniconda3/lib/python3.12/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (14.2.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /home/thanh/miniconda3/lib/python3.12/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.8.0)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /home/thanh/miniconda3/lib/python3.12/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.4.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/thanh/miniconda3/lib/python3.12/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/thanh/miniconda3/lib/python3.12/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/thanh/miniconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
      "Collecting xformers<0.0.27\n",
      "  Using cached xformers-0.0.26.post1.tar.gz (4.1 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m√ó\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m‚ï∞‚îÄ>\u001b[0m \u001b[31m[20 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/home/thanh/miniconda3/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 389, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/thanh/miniconda3/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 373, in main\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/thanh/miniconda3/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 143, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-razhkm6d/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 331, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return self._get_build_requires(config_settings, requirements=[])\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-razhkm6d/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 301, in _get_build_requires\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-razhkm6d/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 512, in run_setup\n",
      "  \u001b[31m   \u001b[0m     super().run_setup(setup_script=setup_script)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-razhkm6d/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 317, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 23, in <module>\n",
      "  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'torch'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[31mERROR: Failed to build 'xformers' when getting requirements to build wheel\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Cell 1: C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "# Unsloth: Framework t·ªëi ∆∞u cho vi·ªác finetune LLM models nhanh h∆°n v√† ti·∫øt ki·ªám memory\n",
    "# xformers: T·ªëi ∆∞u h√≥a transformer operations\n",
    "# trl: Transformer Reinforcement Learning - th∆∞ vi·ªán cho supervised fine-tuning\n",
    "# peft: Parameter-Efficient Fine-Tuning - ch·ªâ train m·ªôt ph·∫ßn nh·ªè parameters\n",
    "# accelerate: TƒÉng t·ªëc training tr√™n multi-GPU\n",
    "# bitsandbytes: Quantization library ƒë·ªÉ gi·∫£m memory footprint\n",
    "\n",
    "! pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "! pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85a1798",
   "metadata": {},
   "source": [
    "### Import th∆∞ vi·ªán v√† load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b3cdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanh/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "T·ªïng s·ªë c√¢u h·ªèi-tr·∫£ l·ªùi: 199\n",
      "V√≠ d·ª• ƒë·∫ßu ti√™n: {'instruction': 'B·∫°n t√™n l√† g√¨?', 'input': '', 'output': 'T√¥i t√™n l√† Nguy·ªÖn Tu·∫•n Thanh, m·ªôt Backend Engineer v√† AI Engineer ƒëang sinh s·ªëng t·∫°i C·∫ßu Gi·∫•y, H√† N·ªôi.'}\n",
      "\n",
      "Dataset info: Dataset({\n",
      "    features: ['instruction', 'input', 'output'],\n",
      "    num_rows: 199\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load d·ªØ li·ªáu t·ª´ file JSON\n",
    "with open('data.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Ki·ªÉm tra s·ªë l∆∞·ª£ng samples\n",
    "print(f\"T·ªïng s·ªë c√¢u h·ªèi-tr·∫£ l·ªùi: {len(data)}\")\n",
    "print(f\"V√≠ d·ª• ƒë·∫ßu ti√™n: {data[0]}\")\n",
    "\n",
    "# Convert sang Hugging Face Dataset format ƒë·ªÉ d·ªÖ x·ª≠ l√Ω\n",
    "dataset = Dataset.from_list(data)\n",
    "print(f\"\\nDataset info: {dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812e1a8c",
   "metadata": {},
   "source": [
    "### C·∫•u h√¨nh v√† load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb8a7a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load model Gemma-2-9B...\n",
      "‚è≥ Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t 2-5 ph√∫t...\n",
      "==((====))==  Unsloth 2025.11.6: Fast Gemma2 patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "‚úÖ Model loaded successfully!\n",
      "Model size: 5.08B parameters\n"
     ]
    }
   ],
   "source": [
    "# max_seq_length: ƒê·ªô d√†i t·ªëi ƒëa c·ªßa sequence (input + output)\n",
    "# - TƒÉng l√™n n·∫øu c√¢u tr·∫£ l·ªùi d√†i, nh∆∞ng t·ªën memory nhi·ªÅu h∆°n\n",
    "# - 2048 tokens ‚âà 1500 words ti·∫øng Anh ho·∫∑c t∆∞∆°ng ƒë∆∞∆°ng\n",
    "max_seq_length = 2048\n",
    "\n",
    "# dtype: Ki·ªÉu d·ªØ li·ªáu cho model weights\n",
    "# - None: Auto-detect (Float16 cho GPU c≈©, Bfloat16 cho GPU m·ªõi)\n",
    "# - Float16: Ti·∫øt ki·ªám memory nh∆∞ng c√≥ th·ªÉ b·ªã numerical instability\n",
    "# - Bfloat16: T·ªët h∆°n cho training stability (c·∫ßn GPU Ampere tr·ªü l√™n)\n",
    "dtype = None  \n",
    "\n",
    "# load_in_4bit: Quantization 4-bit\n",
    "# - True: Gi·∫£m memory usage xu·ªëng ~4x (12GB model ‚Üí 3GB)\n",
    "# - Cho ph√©p train model l·ªõn tr√™n GPU nh·ªè\n",
    "# - Trade-off: Gi·∫£m nh·∫π accuracy nh∆∞ng th∆∞·ªùng kh√¥ng ƒë√°ng k·ªÉ\n",
    "load_in_4bit = True  \n",
    "\n",
    "print(\"ƒêang load model Gemma-2-9B...\")\n",
    "print(\"‚è≥ Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t 2-5 ph√∫t...\")\n",
    "\n",
    "# Load pre-trained model v√† tokenizer\n",
    "# unsloth/gemma-2-9b-bnb-4bit: Gemma-2-9B ƒë√£ ƒë∆∞·ª£c quantize 4-bit s·∫µn\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"Model size: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b855d69d",
   "metadata": {},
   "source": [
    "###  Th√™m LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd5b743a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang th√™m LoRA adapters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.11.6 patched 42 layers with 42 QKV layers, 42 O layers and 42 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LoRA adapters added!\n",
      "Trainable parameters: 54,018,048 (1.05%)\n",
      "Total parameters: 5,133,925,888\n"
     ]
    }
   ],
   "source": [
    "# LoRA l√† k·ªπ thu·∫≠t PEFT - ch·ªâ train th√™m m·ªôt s·ªë parameters nh·ªè thay v√¨ to√†n b·ªô model\n",
    "\n",
    "print(\"ƒêang th√™m LoRA adapters...\")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    \n",
    "    # r: Rank c·ªßa LoRA matrices (low-rank decomposition)\n",
    "    # - Gi√° tr·ªã nh·ªè h∆°n ‚Üí √≠t parameters h∆°n ‚Üí train nhanh h∆°n nh∆∞ng capacity th·∫•p\n",
    "    # - Gi√° tr·ªã l·ªõn h∆°n ‚Üí nhi·ªÅu parameters ‚Üí h·ªçc ƒë∆∞·ª£c nhi·ªÅu h∆°n nh∆∞ng train l√¢u\n",
    "    # - Recommended: 8-64 (16 l√† sweet spot cho most cases)\n",
    "    r=16,\n",
    "    \n",
    "    # target_modules: C√°c layers s·∫Ω apply LoRA\n",
    "    # - q_proj, k_proj, v_proj: Query, Key, Value projections trong attention\n",
    "    # - o_proj: Output projection c·ªßa attention\n",
    "    # - gate_proj, up_proj, down_proj: MLP layers\n",
    "    # - Train c√†ng nhi·ªÅu modules ‚Üí h·ªçc ƒë∆∞·ª£c nhi·ªÅu h∆°n nh∆∞ng t·ªën memory\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    \n",
    "    # lora_alpha: Scaling factor cho LoRA weights\n",
    "    # - Th∆∞·ªùng set b·∫±ng r ho·∫∑c 2*r\n",
    "    # - Cao h∆°n ‚Üí LoRA c√≥ impact m·∫°nh h∆°n l√™n model\n",
    "    lora_alpha=16,\n",
    "    \n",
    "    # lora_dropout: Dropout rate cho LoRA layers\n",
    "    # - 0: Kh√¥ng dropout ‚Üí train nhanh nh·∫•t (optimized by Unsloth)\n",
    "    # - 0.05-0.1: Gi√∫p prevent overfitting n·∫øu dataset nh·ªè\n",
    "    lora_dropout=0,\n",
    "    \n",
    "    # bias: C√≥ train bias terms kh√¥ng\n",
    "    # - \"none\": Kh√¥ng train bias (fastest, recommended)\n",
    "    # - \"all\": Train t·∫•t c·∫£ biases\n",
    "    # - \"lora_only\": Ch·ªâ train biases c·ªßa LoRA layers\n",
    "    bias=\"none\",\n",
    "    \n",
    "    # use_gradient_checkpointing: Ti·∫øt ki·ªám memory b·∫±ng c√°ch recompute\n",
    "    # - \"unsloth\": Unsloth's optimized checkpointing (recommended)\n",
    "    # - True: Standard gradient checkpointing\n",
    "    # - False: Kh√¥ng d√πng (nhanh nh∆∞ng t·ªën memory)\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    \n",
    "    # random_state: Seed cho reproducibility\n",
    "    random_state=3407,\n",
    "    \n",
    "    # use_rslora: Rank-Stabilized LoRA\n",
    "    # - False: Standard LoRA (recommended for most cases)\n",
    "    # - True: Better stability v·ªõi high rank values\n",
    "    use_rslora=False,\n",
    "    \n",
    "    # loftq_config: LoftQ initialization\n",
    "    # - None: Standard initialization (recommended)\n",
    "    # - Config object: Special init for quantized models\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "# In ra s·ªë l∆∞·ª£ng trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"‚úÖ LoRA adapters added!\")\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeac4c44",
   "metadata": {},
   "source": [
    "### Format d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4a6dc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang format dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [00:00<00:00, 9485.82 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset formatted!\n",
      "Sample formatted text:\n",
      "D∆∞·ªõi ƒë√¢y l√† m·ªôt c√¢u h·ªèi v·ªÅ Nguy·ªÖn Tu·∫•n Thanh. H√£y tr·∫£ l·ªùi ch√≠nh x√°c v√† chi ti·∫øt.\n",
      "\n",
      "### C√¢u h·ªèi:\n",
      "B·∫°n t√™n l√† g√¨?\n",
      "\n",
      "### Ng·ªØ c·∫£nh th√™m:\n",
      "\n",
      "\n",
      "### Tr·∫£ l·ªùi:\n",
      "T√¥i t√™n l√† Nguy·ªÖn Tu·∫•n Thanh, m·ªôt Backend Engineer v√† AI Engineer ƒëang sinh s·ªëng t·∫°i C·∫ßu Gi·∫•y, H√† N·ªôi.<eos>...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Format data cho training\n",
    "# T·∫°o prompt template ph√π h·ª£p v·ªõi task Q&A v·ªÅ b·∫°n\n",
    "\n",
    "# EOS_TOKEN: End of Sequence token - ƒë√°nh d·∫•u k·∫øt th√∫c c√¢u tr·∫£ l·ªùi\n",
    "# CRITICAL: Ph·∫£i th√™m EOS_TOKEN ƒë·ªÉ model bi·∫øt khi n√†o d·ª´ng generate\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# Template cho prompt - customize theo style b·∫°n mu·ªën\n",
    "# Format: Instruction-based prompting\n",
    "alpaca_prompt = \"\"\"D∆∞·ªõi ƒë√¢y l√† m·ªôt c√¢u h·ªèi v·ªÅ Nguy·ªÖn Tu·∫•n Thanh. H√£y tr·∫£ l·ªùi ch√≠nh x√°c v√† chi ti·∫øt.\n",
    "\n",
    "### C√¢u h·ªèi:\n",
    "{}\n",
    "\n",
    "### Ng·ªØ c·∫£nh th√™m:\n",
    "{}\n",
    "\n",
    "### Tr·∫£ l·ªùi:\n",
    "{}\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"\n",
    "    H√†m format data th√†nh prompt strings\n",
    "    \n",
    "    Args:\n",
    "        examples: Dict ch·ª©a c√°c lists c·ªßa instruction, input, output\n",
    "    \n",
    "    Returns:\n",
    "        Dict v·ªõi key \"text\" ch·ª©a formatted prompts\n",
    "    \"\"\"\n",
    "    instructions = examples[\"instruction\"]  # C√¢u h·ªèi\n",
    "    inputs = examples[\"input\"]              # Context th√™m (c√≥ th·ªÉ r·ªóng)\n",
    "    outputs = examples[\"output\"]            # C√¢u tr·∫£ l·ªùi\n",
    "    texts = []\n",
    "    \n",
    "    for instruction, inp, output in zip(instructions, inputs, outputs):\n",
    "        # Format theo template v√† th√™m EOS_TOKEN\n",
    "        # EOS_TOKEN gi√∫p model bi·∫øt khi n√†o stop generating\n",
    "        text = alpaca_prompt.format(instruction, inp, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting function l√™n to√†n b·ªô dataset\n",
    "# batched=True: X·ª≠ l√Ω nhi·ªÅu examples c√πng l√∫c (nhanh h∆°n)\n",
    "print(\"ƒêang format dataset...\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(\"‚úÖ Dataset formatted!\")\n",
    "print(f\"Sample formatted text:\\n{dataset[0]['text'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3a60f",
   "metadata": {},
   "source": [
    "### C·∫•u h√¨nh training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d8e75d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training arguments configured!\n",
      "Effective batch size: 8\n",
      "Total training steps: 200\n",
      "Learning rate: 0.0002\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    # per_device_train_batch_size: S·ªë samples per GPU per step\n",
    "    # - TƒÉng l√™n ƒë·ªÉ train nhanh h∆°n (n·∫øu ƒë·ªß memory)\n",
    "    # - Gi·∫£m xu·ªëng n·∫øu b·ªã OOM (Out of Memory)\n",
    "    # - Recommended: 2-8 cho 4-bit quantized models\n",
    "    per_device_train_batch_size=2,\n",
    "    \n",
    "    # gradient_accumulation_steps: T√≠ch l≈©y gradients qua N steps\n",
    "    # - Effective batch size = per_device_batch_size * gradient_accumulation_steps * num_gpus\n",
    "    # - TƒÉng l√™n ƒë·ªÉ simulate batch size l·ªõn h∆°n m√† kh√¥ng t·ªën memory\n",
    "    # - V√≠ d·ª•: 2 * 4 = effective batch size 8\n",
    "    gradient_accumulation_steps=4,\n",
    "    \n",
    "    # warmup_steps: S·ªë steps ƒë·ªÉ tƒÉng learning rate t·ª´ 0 l√™n max\n",
    "    # - Gi√∫p training stable h∆°n ·ªü ƒë·∫ßu\n",
    "    # - Recommended: 5-10% total steps\n",
    "    # - V√≠ d·ª•: 5-10 steps cho small training runs\n",
    "    warmup_steps=10,\n",
    "    \n",
    "    # max_steps: T·ªïng s·ªë training steps\n",
    "    # - 1 step = 1 forward + 1 backward pass\n",
    "    # - TƒÉng l√™n ƒë·ªÉ train l√¢u h∆°n (better results nh∆∞ng risk overfit)\n",
    "    # - Recommended: 100-500 steps cho dataset 300 samples\n",
    "    # - Set -1 ƒë·ªÉ train theo num_train_epochs thay v√¨ steps\n",
    "    max_steps=200,  # TƒÉng t·ª´ 60 l√™n 200 ƒë·ªÉ h·ªçc t·ªët h∆°n\n",
    "    \n",
    "    # learning_rate: T·ªëc ƒë·ªô h·ªçc (quan tr·ªçng nh·∫•t!)\n",
    "    # - Qu√° l·ªõn ‚Üí model kh√¥ng converge, loss oscillate\n",
    "    # - Qu√° nh·ªè ‚Üí h·ªçc ch·∫≠m, c√≥ th·ªÉ stuck\n",
    "    # - Recommended cho LoRA: 1e-4 ƒë·∫øn 5e-4\n",
    "    # - 2e-4 l√† good default\n",
    "    learning_rate=2e-4,\n",
    "    \n",
    "    # fp16 / bf16: Mixed precision training\n",
    "    # - fp16: D√πng cho GPU c≈© (Tesla T4, V100)\n",
    "    # - bf16: D√πng cho GPU m·ªõi (A100, H100, RTX 30/40 series)\n",
    "    # - Gi·∫£m memory usage v√† tƒÉng speed ~2x\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    \n",
    "    # logging_steps: Log metrics sau m·ªói N steps\n",
    "    # - 1: Log m·ªçi step (chi ti·∫øt nh·∫•t)\n",
    "    # - 10: Log m·ªói 10 steps (√≠t spam h∆°n)\n",
    "    logging_steps=5,\n",
    "    \n",
    "    # optim: Optimizer algorithm\n",
    "    # - \"adamw_8bit\": AdamW v·ªõi 8-bit quantization (ti·∫øt ki·ªám memory)\n",
    "    # - \"adamw_torch\": Standard AdamW\n",
    "    # - \"sgd\": Stochastic Gradient Descent (√≠t d√πng)\n",
    "    optim=\"adamw_8bit\",\n",
    "    \n",
    "    # weight_decay: L2 regularization\n",
    "    # - Gi√∫p prevent overfitting\n",
    "    # - Recommended: 0.01-0.1\n",
    "    # - 0: Kh√¥ng regularization\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # lr_scheduler_type: Learning rate schedule\n",
    "    # - \"linear\": Gi·∫£m d·∫ßn t·ª´ max xu·ªëng 0 (recommended)\n",
    "    # - \"cosine\": Gi·∫£m theo cosine curve\n",
    "    # - \"constant\": Gi·ªØ nguy√™n learning rate\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    \n",
    "    # seed: Random seed cho reproducibility\n",
    "    # - Set gi·ªëng nhau ƒë·ªÉ results gi·ªëng nhau m·ªói run\n",
    "    seed=3407,\n",
    "    \n",
    "    # output_dir: Th∆∞ m·ª•c save checkpoints\n",
    "    output_dir=\"outputs\",\n",
    "    \n",
    "    # report_to: N∆°i log metrics\n",
    "    # - \"none\": Kh√¥ng log external (ch·ªâ console)\n",
    "    # - \"wandb\": Log l√™n Weights & Biases\n",
    "    # - \"tensorboard\": Log l√™n TensorBoard\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # save_strategy: Khi n√†o save checkpoint\n",
    "    # - \"steps\": Save m·ªói save_steps\n",
    "    # - \"epoch\": Save cu·ªëi m·ªói epoch\n",
    "    # - \"no\": Kh√¥ng save (ch·ªâ save final)\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,  # Save m·ªói 50 steps\n",
    "    \n",
    "    # eval_strategy: Khi n√†o evaluate (n·∫øu c√≥ eval dataset)\n",
    "    # - \"steps\": Eval m·ªói eval_steps\n",
    "    # - \"epoch\": Eval cu·ªëi epoch\n",
    "    # - \"no\": Kh√¥ng eval\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=50,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured!\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Total training steps: {training_args.max_steps}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccaae16",
   "metadata": {},
   "source": [
    "### Kh·ªüi t·∫°o Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99984955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=12): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [00:10<00:00, 19.52 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer initialized!\n",
      "Number of training samples: 199\n",
      "Estimated training time: ~6.7 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    # model: Model v·ªõi LoRA adapters\n",
    "    model=model,\n",
    "    \n",
    "    # tokenizer: ƒê·ªÉ tokenize text th√†nh numbers\n",
    "    tokenizer=tokenizer,\n",
    "    \n",
    "    # train_dataset: Dataset ƒë√£ format\n",
    "    train_dataset=dataset,\n",
    "    \n",
    "    # dataset_text_field: T√™n column ch·ª©a text ƒë√£ format\n",
    "    # - Ph·∫£i match v·ªõi output c·ªßa formatting_prompts_func\n",
    "    dataset_text_field=\"text\",\n",
    "    \n",
    "    # max_seq_length: Max length t·ª´ model config\n",
    "    max_seq_length=max_seq_length,\n",
    "    \n",
    "    # dataset_num_proc: S·ªë processes cho data processing\n",
    "    # - TƒÉng l√™n ƒë·ªÉ preprocess nhanh h∆°n (n·∫øu c√≥ nhi·ªÅu CPU cores)\n",
    "    # - 2-4 l√† reasonable\n",
    "    dataset_num_proc=2,\n",
    "    \n",
    "    # packing: Pack nhi·ªÅu short sequences v√†o m·ªôt sequence\n",
    "    # - True: Hi·ªáu qu·∫£ h∆°n cho short texts (tƒÉng throughput 5x)\n",
    "    # - False: An to√†n h∆°n, kh√¥ng mix different conversations\n",
    "    # - Recommended: False cho Q&A tasks ƒë·ªÉ tr√°nh confusion\n",
    "    packing=False,\n",
    "    \n",
    "    # args: Training arguments t·ª´ cell tr∆∞·ªõc\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized!\")\n",
    "print(f\"Number of training samples: {len(dataset)}\")\n",
    "print(f\"Estimated training time: ~{training_args.max_steps * 2 / 60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85dcf2",
   "metadata": {},
   "source": [
    "### B·∫Øt ƒë·∫ßu training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0661e0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üöÄ B·∫ÆT ƒê·∫¶U TRAINING!\n",
      "==================================================\n",
      "Dataset size: 199 samples\n",
      "Training steps: 200\n",
      "Batch size: 2\n",
      "Learning rate: 0.0002\n",
      "==================================================\n",
      "\n",
      "‚è≥ Training in progress... H√£y ki√™n nh·∫´n!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 199 | Num Epochs = 8 | Total steps = 200\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 54,018,048 of 9,295,724,032 (0.58% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 18:20, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.316000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.825300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.191400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.970400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.921100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.663500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.667400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.634100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.571600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.337400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.320100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.314200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.319300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.176700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.166900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.164800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.177800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.150500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.139100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.091300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.089500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.099600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.092300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.101700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.076600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.080200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.081200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.080100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.069600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.071400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.072300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.071800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.074700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 55fb8e21-363d-406b-95cf-bbd5e63026d9)')' thrown while requesting HEAD https://huggingface.co/unsloth/gemma-2-9b-bnb-4bit/resolve/main/config.json\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 55fb8e21-363d-406b-95cf-bbd5e63026d9)')' thrown while requesting HEAD https://huggingface.co/unsloth/gemma-2-9b-bnb-4bit/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 123357fc-2e2a-4220-bc3f-dbeae703d618)')' thrown while requesting HEAD https://huggingface.co/unsloth/gemma-2-9b-bnb-4bit/resolve/main/config.json\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 123357fc-2e2a-4220-bc3f-dbeae703d618)')' thrown while requesting HEAD https://huggingface.co/unsloth/gemma-2-9b-bnb-4bit/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: a2784f24-414d-4acc-b305-b8ff1de096eb)')' thrown while requesting HEAD https://huggingface.co/unsloth/gemma-2-9b-bnb-4bit/resolve/main/config.json\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: a2784f24-414d-4acc-b305-b8ff1de096eb)')' thrown while requesting HEAD https://huggingface.co/unsloth/gemma-2-9b-bnb-4bit/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 493bef51-054a-48e6-afe3-166b56908ddd)')' thrown while requesting HEAD https://huggingface.co/unsloth/gemma-2-9b-bnb-4bit/resolve/main/config.json\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 493bef51-054a-48e6-afe3-166b56908ddd)')' thrown while requesting HEAD https://huggingface.co/unsloth/gemma-2-9b-bnb-4bit/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "‚úÖ TRAINING HO√ÄN T·∫§T!\n",
      "==================================================\n",
      "Final loss: 0.3948\n",
      "Training time: 1151.40 seconds\n",
      "Samples per second: 1.39\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"üöÄ B·∫ÆT ƒê·∫¶U TRAINING!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset size: {len(dataset)} samples\")\n",
    "print(f\"Training steps: {training_args.max_steps}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\n‚è≥ Training in progress... H√£y ki√™n nh·∫´n!\\n\")\n",
    "\n",
    "# Train!\n",
    "# Qu√° tr√¨nh n√†y s·∫Ω:\n",
    "# 1. Load batches of data\n",
    "# 2. Forward pass (t√≠nh predictions)\n",
    "# 3. Compute loss (sai s·ªë gi·ªØa prediction v√† actual)\n",
    "# 4. Backward pass (t√≠nh gradients)\n",
    "# 5. Update weights\n",
    "# 6. Repeat\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ TRAINING HO√ÄN T·∫§T!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Final loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Samples per second: {trainer_stats.metrics['train_samples_per_second']:.2f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb34973",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99fea8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang test model...\n",
      "\n",
      "==================================================\n",
      "üß™ TESTING MODEL\n",
      "==================================================\n",
      "\n",
      "‚ùì C√¢u h·ªèi: B·∫°n h√£y gi·ªõi thi·ªáu c√°c th√¥ng tin v·ªÅ b·∫°n ƒëi nh·ªõ gi·ªõi thi·ªáu kƒ© v√† d√†i v√†o bao g·ªìm s·ªü th√≠ch, c√°c d·ª± √°n ƒë√£ l√†m, kƒ© nƒÉng ƒë√£ t√≠ch l≈©y.\n",
      "üí¨ Tr·∫£ l·ªùi: T√¥i t√™n l√† Nguy·ªÖn Tu·∫•n Thanh, m·ªôt Backend Engineer v√† AI Engineer ƒëang sinh s·ªëng t·∫°i C·∫ßu Gi·∫•y, H√† N·ªôi. S·ªü th√≠ch c·ªßa t√¥i l√† research AI/ML papers m·ªõi, contribute to open-source projects, v√† build scalable backend systems.\n",
      "--------------------------------------------------\n",
      "\n",
      "‚úÖ Testing complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang test model...\")\n",
    "\n",
    "# Enable inference mode (faster, no gradient computation)\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Danh s√°ch c√¢u h·ªèi test\n",
    "test_questions = [\n",
    "    # \"B·∫°n t√™n l√† g√¨?\",\n",
    "    # \"B·∫°n l√†m vi·ªác ·ªü ƒë√¢u?\",\n",
    "    # \"K·ªπ nƒÉng c·ªßa b·∫°n l√† g√¨?\",\n",
    "    # \"D·ª± √°n n√†o b·∫°n t·ª± h√†o nh·∫•t?\",\n",
    "    # \"B·∫°n h·ªçc tr∆∞·ªùng n√†o?\"\n",
    "    \"B·∫°n h√£y gi·ªõi thi·ªáu c√°c th√¥ng tin v·ªÅ b·∫°n ƒëi nh·ªõ gi·ªõi thi·ªáu kƒ© v√† d√†i v√†o bao g·ªìm s·ªü th√≠ch, c√°c d·ª± √°n ƒë√£ l√†m, kƒ© nƒÉng ƒë√£ t√≠ch l≈©y.\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üß™ TESTING MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for question in test_questions:\n",
    "    # Format theo template\n",
    "    prompt = alpaca_prompt.format(\n",
    "        question,  # instruction\n",
    "        \"\",        # input (empty)\n",
    "        \"\"         # output (ƒë·ªÉ model generate)\n",
    "    )\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        [prompt], \n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Generate response\n",
    "    # max_new_tokens: S·ªë tokens t·ªëi ƒëa cho response\n",
    "    # temperature: 0.7 = creative h∆°n, 0.1 = deterministic h∆°n\n",
    "    # top_p: Nucleus sampling (0.9 l√† good default)\n",
    "    # do_sample: True = sample t·ª´ distribution, False = greedy (ch·ªçn highest prob)\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        use_cache=True  # Cache key/value trong attention ƒë·ªÉ generate nhanh h∆°n\n",
    "    )\n",
    "    \n",
    "    # Decode output\n",
    "    generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Extract only the response part (after \"### Tr·∫£ l·ªùi:\")\n",
    "    response = generated_text.split(\"### Tr·∫£ l·ªùi:\")[-1].strip()\n",
    "    \n",
    "    print(f\"\\n‚ùì C√¢u h·ªèi: {question}\")\n",
    "    print(f\"üí¨ Tr·∫£ l·ªùi: {response}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "print(\"\\n‚úÖ Testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2532aab7",
   "metadata": {},
   "source": [
    "### L∆∞u model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a55c38d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang l∆∞u model...\n",
      "‚úÖ Saved LoRA adapters to gemma2_thanh_lora/\n",
      "\n",
      "üìÅ Files saved:\n",
      "  - adapter_config.json: LoRA configuration\n",
      "  - adapter_model.safetensors: LoRA weights\n",
      "  - tokenizer files: ƒê·ªÉ encode/decode text\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang l∆∞u model...\")\n",
    "\n",
    "# Option 1: L∆∞u ch·ªâ LoRA adapters (nh·∫π nh·∫•t, ~10-50MB)\n",
    "# Khi load l·∫°i c·∫ßn base model + adapters n√†y\n",
    "model.save_pretrained(\"gemma2_thanh_lora\")\n",
    "tokenizer.save_pretrained(\"gemma2_thanh_lora\")\n",
    "print(\"‚úÖ Saved LoRA adapters to gemma2_thanh_lora/\")\n",
    "\n",
    "# Option 2: L∆∞u merged model (base + LoRA) ·ªü 16-bit\n",
    "# File size l·ªõn h∆°n (~18GB) nh∆∞ng d·ªÖ d√πng h∆°n\n",
    "# model.save_pretrained_merged(\n",
    "#     \"gemma2_thanh_merged_16bit\",\n",
    "#     tokenizer,\n",
    "#     save_method=\"merged_16bit\"\n",
    "# )\n",
    "# print(\"‚úÖ Saved merged 16-bit model to gemma2_thanh_merged_16bit/\")\n",
    "\n",
    "# Option 3: L∆∞u merged + quantized v·ªÅ 4-bit (balance size v√† quality)\n",
    "# model.save_pretrained_merged(\n",
    "#     \"gemma2_thanh_merged_4bit\",\n",
    "#     tokenizer,\n",
    "#     save_method=\"merged_4bit\"\n",
    "# )\n",
    "# print(\"‚úÖ Saved merged 4-bit model to gemma2_thanh_merged_4bit/\")\n",
    "\n",
    "print(\"\\nüìÅ Files saved:\")\n",
    "print(\"  - adapter_config.json: LoRA configuration\")\n",
    "print(\"  - adapter_model.safetensors: LoRA weights\")\n",
    "print(\"  - tokenizer files: ƒê·ªÉ encode/decode text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3f57cc",
   "metadata": {},
   "source": [
    "### Push to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a8db3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C·∫ßn Hugging Face account v√† access token\n",
    "\n",
    "# Uncomment v√† fill in your info ƒë·ªÉ push\n",
    "\"\"\"\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login to HuggingFace\n",
    "# Get token from: https://huggingface.co/settings/tokens\n",
    "login(token=\"YOUR_HF_TOKEN_HERE\")\n",
    "\n",
    "# Push LoRA adapters\n",
    "model.push_to_hub(\n",
    "    \"ntthanh2603/gemma2-thanh-lora\",  # Repo name\n",
    "    token=\"YOUR_HF_TOKEN_HERE\",\n",
    "    private=False  # False = public, True = private\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    \"ntthanh2603/gemma2-thanh-lora\",\n",
    "    token=\"YOUR_HF_TOKEN_HERE\"\n",
    ")\n",
    "\n",
    "print(\"Model pushed to HuggingFace Hub!\")\n",
    "print(\"View at: https://huggingface.co/ntthanh2603/gemma2-thanh-lora\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffa322f",
   "metadata": {},
   "source": [
    "### Load model ƒë√£ save ƒë·ªÉ d√πng l·∫°i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e59f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Khi mu·ªën load model ƒë√£ save:\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Load base model + LoRA adapters\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"gemma2_thanh_lora\",  # Path ƒë·∫øn folder ƒë√£ save\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Gi·ªù c√≥ th·ªÉ d√πng ƒë·ªÉ generate nh∆∞ b√¨nh th∆∞·ªùng!\n",
    "\"\"\"\n",
    "\n",
    "print(\"Code tr√™n ƒë·ªÉ load model ƒë√£ save\")\n",
    "print(\"Copy v√† run trong notebook m·ªõi khi mu·ªën d√πng model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
